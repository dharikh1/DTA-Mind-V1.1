{
  "AgentFlow": [],
  "AgentFlowV2": [
    {
      "id": "a324a062-bff8-4e4e-b612-9d55c51a193b",
      "name": "T1",
      "flowData": "{\n  \"nodes\": [\n    {\n      \"id\": \"startAgentflow_0\",\n      \"type\": \"agentFlow\",\n      \"position\": {\n        \"x\": -306.1899357911624,\n        \"y\": 100.66096874816344\n      },\n      \"data\": {\n        \"id\": \"startAgentflow_0\",\n        \"label\": \"Start\",\n        \"version\": 1.1,\n        \"name\": \"startAgentflow\",\n        \"type\": \"Start\",\n        \"color\": \"#7EE787\",\n        \"hideInput\": true,\n        \"baseClasses\": [\n          \"Start\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Starting point of the agentflow\",\n        \"inputParams\": [\n          {\n            \"label\": \"Input Type\",\n            \"name\": \"startInputType\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"Chat Input\",\n                \"name\": \"chatInput\",\n                \"description\": \"Start the conversation with chat input\"\n              },\n              {\n                \"label\": \"Form Input\",\n                \"name\": \"formInput\",\n                \"description\": \"Start the workflow with form inputs\"\n              }\n            ],\n            \"default\": \"chatInput\",\n            \"id\": \"startAgentflow_0-input-startInputType-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Form Title\",\n            \"name\": \"formTitle\",\n            \"type\": \"string\",\n            \"placeholder\": \"Please Fill Out The Form\",\n            \"show\": {\n              \"startInputType\": \"formInput\"\n            },\n            \"id\": \"startAgentflow_0-input-formTitle-string\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Form Description\",\n            \"name\": \"formDescription\",\n            \"type\": \"string\",\n            \"placeholder\": \"Complete all fields below to continue\",\n            \"show\": {\n              \"startInputType\": \"formInput\"\n            },\n            \"id\": \"startAgentflow_0-input-formDescription-string\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Form Input Types\",\n            \"name\": \"formInputTypes\",\n            \"description\": \"Specify the type of form input\",\n            \"type\": \"array\",\n            \"show\": {\n              \"startInputType\": \"formInput\"\n            },\n            \"array\": [\n              {\n                \"label\": \"Type\",\n                \"name\": \"type\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"String\",\n                    \"name\": \"string\"\n                  },\n                  {\n                    \"label\": \"Number\",\n                    \"name\": \"number\"\n                  },\n                  {\n                    \"label\": \"Boolean\",\n                    \"name\": \"boolean\"\n                  },\n                  {\n                    \"label\": \"Options\",\n                    \"name\": \"options\"\n                  }\n                ],\n                \"default\": \"string\"\n              },\n              {\n                \"label\": \"Label\",\n                \"name\": \"label\",\n                \"type\": \"string\",\n                \"placeholder\": \"Label for the input\"\n              },\n              {\n                \"label\": \"Variable Name\",\n                \"name\": \"name\",\n                \"type\": \"string\",\n                \"placeholder\": \"Variable name for the input (must be camel case)\",\n                \"description\": \"Variable name must be camel case. For example: firstName, lastName, etc.\"\n              },\n              {\n                \"label\": \"Add Options\",\n                \"name\": \"addOptions\",\n                \"type\": \"array\",\n                \"show\": {\n                  \"formInputTypes[$index].type\": \"options\"\n                },\n                \"array\": [\n                  {\n                    \"label\": \"Option\",\n                    \"name\": \"option\",\n                    \"type\": \"string\"\n                  }\n                ]\n              }\n            ],\n            \"id\": \"startAgentflow_0-input-formInputTypes-array\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Ephemeral Memory\",\n            \"name\": \"startEphemeralMemory\",\n            \"type\": \"boolean\",\n            \"description\": \"Start fresh for every execution without past chat history\",\n            \"optional\": true\n          },\n          {\n            \"label\": \"Flow State\",\n            \"name\": \"startState\",\n            \"description\": \"Runtime state during the execution of the workflow\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"string\",\n                \"placeholder\": \"Foo\"\n              },\n              {\n                \"label\": \"Value\",\n                \"name\": \"value\",\n                \"type\": \"string\",\n                \"placeholder\": \"Bar\"\n              }\n            ],\n            \"id\": \"startAgentflow_0-input-startState-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Persist State\",\n            \"name\": \"startPersistState\",\n            \"type\": \"boolean\",\n            \"description\": \"Persist the state in the same session\",\n            \"optional\": true,\n            \"id\": \"startAgentflow_0-input-startPersistState-boolean\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"startInputType\": \"chatInput\",\n          \"formTitle\": \"\",\n          \"formDescription\": \"\",\n          \"formInputTypes\": \"\",\n          \"startState\": [\n            {\n              \"key\": \"query\",\n              \"value\": \"\"\n            }\n          ]\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"startAgentflow_0-output-startAgentflow\",\n            \"label\": \"Start\",\n            \"name\": \"startAgentflow\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"width\": 120,\n      \"height\": 146,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": -306.1899357911624,\n        \"y\": 100.66096874816344\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"conditionAgentAgentflow_0\",\n      \"position\": {\n        \"x\": -149.92594008068767,\n        \"y\": -145.54968104809635\n      },\n      \"data\": {\n        \"id\": \"conditionAgentAgentflow_0\",\n        \"label\": \"Check if query valid\",\n        \"version\": 1,\n        \"name\": \"conditionAgentAgentflow\",\n        \"type\": \"ConditionAgent\",\n        \"color\": \"#ff8fab\",\n        \"baseClasses\": [\n          \"ConditionAgent\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Utilize an agent to split flows based on dynamic conditions\",\n        \"inputParams\": [\n          {\n            \"label\": \"Model\",\n            \"name\": \"conditionAgentModel\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"loadConfig\": true,\n            \"id\": \"conditionAgentAgentflow_0-input-conditionAgentModel-asyncOptions\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Instructions\",\n            \"name\": \"conditionAgentInstructions\",\n            \"type\": \"string\",\n            \"description\": \"A general instructions of what the condition agent should do\",\n            \"rows\": 4,\n            \"acceptVariable\": true,\n            \"placeholder\": \"Determine if the user is interested in learning about AI\",\n            \"id\": \"conditionAgentAgentflow_0-input-conditionAgentInstructions-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Input\",\n            \"name\": \"conditionAgentInput\",\n            \"type\": \"string\",\n            \"description\": \"Input to be used for the condition agent\",\n            \"rows\": 4,\n            \"acceptVariable\": true,\n            \"default\": \"<p><span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"question\\\" data-label=\\\"question\\\">{{ question }}</span> </p>\",\n            \"id\": \"conditionAgentAgentflow_0-input-conditionAgentInput-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Scenarios\",\n            \"name\": \"conditionAgentScenarios\",\n            \"description\": \"Define the scenarios that will be used as the conditions to split the flow\",\n            \"type\": \"array\",\n            \"array\": [\n              {\n                \"label\": \"Scenario\",\n                \"name\": \"scenario\",\n                \"type\": \"string\",\n                \"placeholder\": \"User is asking for a pizza\"\n              }\n            ],\n            \"default\": [\n              {\n                \"scenario\": \"AI Related\"\n              },\n              {\n                \"scenario\": \"General\"\n              }\n            ],\n            \"id\": \"conditionAgentAgentflow_0-input-conditionAgentScenarios-array\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"conditionAgentModel\": \"chatOpenAI\",\n          \"conditionAgentInstructions\": \"<p>Check if user is asking about AI related topic, or just general query</p>\",\n          \"conditionAgentInput\": \"<p><span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"question\\\" data-label=\\\"question\\\">{{ question }}</span> </p>\",\n          \"conditionAgentScenarios\": [\n            {\n              \"scenario\": \"AI Related\"\n            },\n            {\n              \"scenario\": \"General\"\n            }\n          ],\n          \"conditionAgentModelConfig\": {\n            \"credential\": \"\",\n            \"modelName\": \"gpt-4o-mini\",\n            \"temperature\": 0.9,\n            \"streaming\": true,\n            \"maxTokens\": \"\",\n            \"topP\": \"\",\n            \"frequencyPenalty\": \"\",\n            \"presencePenalty\": \"\",\n            \"timeout\": \"\",\n            \"strictToolCalling\": \"\",\n            \"stopSequence\": \"\",\n            \"basepath\": \"\",\n            \"proxyUrl\": \"\",\n            \"baseOptions\": \"\",\n            \"allowImageUploads\": \"\",\n            \"imageResolution\": \"low\",\n            \"reasoningEffort\": \"\",\n            \"conditionAgentModel\": \"chatOpenAI\"\n          }\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"conditionAgentAgentflow_0-output-0\",\n            \"label\": \"Condition Agent\",\n            \"name\": \"conditionAgentAgentflow\"\n          },\n          {\n            \"id\": \"conditionAgentAgentflow_0-output-1\",\n            \"label\": \"Condition Agent\",\n            \"name\": \"conditionAgentAgentflow\"\n          }\n        ],\n        \"outputs\": {\n          \"conditionAgentAgentflow\": \"\"\n        },\n        \"selected\": false\n      },\n      \"type\": \"agentFlow\",\n      \"width\": 140,\n      \"height\": 106,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": -149.92594008068767,\n        \"y\": -145.54968104809635\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"llmAgentflow_0\",\n      \"position\": {\n        \"x\": 203.99796854914894,\n        \"y\": -282.1573937209969\n      },\n      \"data\": {\n        \"id\": \"llmAgentflow_0\",\n        \"label\": \"Generate Query\",\n        \"version\": 1,\n        \"name\": \"llmAgentflow\",\n        \"type\": \"LLM\",\n        \"color\": \"#64B5F6\",\n        \"baseClasses\": [\n          \"LLM\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Large language models to analyze user-provided inputs and generate responses\",\n        \"inputParams\": [\n          {\n            \"label\": \"Model\",\n            \"name\": \"llmModel\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"loadConfig\": true,\n            \"id\": \"llmAgentflow_0-input-llmModel-asyncOptions\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Messages\",\n            \"name\": \"llmMessages\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Role\",\n                \"name\": \"role\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"System\",\n                    \"name\": \"system\"\n                  },\n                  {\n                    \"label\": \"Assistant\",\n                    \"name\": \"assistant\"\n                  },\n                  {\n                    \"label\": \"Developer\",\n                    \"name\": \"developer\"\n                  },\n                  {\n                    \"label\": \"User\",\n                    \"name\": \"user\"\n                  }\n                ]\n              },\n              {\n                \"label\": \"Content\",\n                \"name\": \"content\",\n                \"type\": \"string\",\n                \"acceptVariable\": true,\n                \"generateInstruction\": true,\n                \"rows\": 4\n              }\n            ],\n            \"id\": \"llmAgentflow_0-input-llmMessages-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Enable Memory\",\n            \"name\": \"llmEnableMemory\",\n            \"type\": \"boolean\",\n            \"description\": \"Enable memory for the conversation thread\",\n            \"default\": true,\n            \"optional\": true,\n            \"id\": \"llmAgentflow_0-input-llmEnableMemory-boolean\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Memory Type\",\n            \"name\": \"llmMemoryType\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"All Messages\",\n                \"name\": \"allMessages\",\n                \"description\": \"Retrieve all messages from the conversation\"\n              },\n              {\n                \"label\": \"Window Size\",\n                \"name\": \"windowSize\",\n                \"description\": \"Uses a fixed window size to surface the last N messages\"\n              },\n              {\n                \"label\": \"Conversation Summary\",\n                \"name\": \"conversationSummary\",\n                \"description\": \"Summarizes the whole conversation\"\n              },\n              {\n                \"label\": \"Conversation Summary Buffer\",\n                \"name\": \"conversationSummaryBuffer\",\n                \"description\": \"Summarize conversations once token limit is reached. Default to 2000\"\n              }\n            ],\n            \"optional\": true,\n            \"default\": \"allMessages\",\n            \"show\": {\n              \"llmEnableMemory\": true\n            },\n            \"id\": \"llmAgentflow_0-input-llmMemoryType-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Window Size\",\n            \"name\": \"llmMemoryWindowSize\",\n            \"type\": \"number\",\n            \"default\": \"20\",\n            \"description\": \"Uses a fixed window size to surface the last N messages\",\n            \"show\": {\n              \"llmMemoryType\": \"windowSize\"\n            },\n            \"id\": \"llmAgentflow_0-input-llmMemoryWindowSize-number\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Max Token Limit\",\n            \"name\": \"llmMemoryMaxTokenLimit\",\n            \"type\": \"number\",\n            \"default\": \"2000\",\n            \"description\": \"Summarize conversations once token limit is reached. Default to 2000\",\n            \"show\": {\n              \"llmMemoryType\": \"conversationSummaryBuffer\"\n            },\n            \"id\": \"llmAgentflow_0-input-llmMemoryMaxTokenLimit-number\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Input Message\",\n            \"name\": \"llmUserMessage\",\n            \"type\": \"string\",\n            \"description\": \"Add an input message as user message at the end of the conversation\",\n            \"rows\": 4,\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"show\": {\n              \"llmEnableMemory\": true\n            },\n            \"id\": \"llmAgentflow_0-input-llmUserMessage-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Return Response As\",\n            \"name\": \"llmReturnResponseAs\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"User Message\",\n                \"name\": \"userMessage\"\n              },\n              {\n                \"label\": \"Assistant Message\",\n                \"name\": \"assistantMessage\"\n              }\n            ],\n            \"default\": \"userMessage\",\n            \"id\": \"llmAgentflow_0-input-llmReturnResponseAs-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"JSON Structured Output\",\n            \"name\": \"llmStructuredOutput\",\n            \"description\": \"Instruct the LLM to give output in a JSON structured schema\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"string\"\n              },\n              {\n                \"label\": \"Type\",\n                \"name\": \"type\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"String\",\n                    \"name\": \"string\"\n                  },\n                  {\n                    \"label\": \"String Array\",\n                    \"name\": \"stringArray\"\n                  },\n                  {\n                    \"label\": \"Number\",\n                    \"name\": \"number\"\n                  },\n                  {\n                    \"label\": \"Boolean\",\n                    \"name\": \"boolean\"\n                  },\n                  {\n                    \"label\": \"Enum\",\n                    \"name\": \"enum\"\n                  },\n                  {\n                    \"label\": \"JSON Array\",\n                    \"name\": \"jsonArray\"\n                  }\n                ]\n              },\n              {\n                \"label\": \"Enum Values\",\n                \"name\": \"enumValues\",\n                \"type\": \"string\",\n                \"placeholder\": \"value1, value2, value3\",\n                \"description\": \"Enum values. Separated by comma\",\n                \"optional\": true,\n                \"show\": {\n                  \"llmStructuredOutput[$index].type\": \"enum\"\n                }\n              },\n              {\n                \"label\": \"JSON Schema\",\n                \"name\": \"jsonSchema\",\n                \"type\": \"code\",\n                \"placeholder\": \"{\\n    \\\"answer\\\": {\\n        \\\"type\\\": \\\"string\\\",\\n        \\\"description\\\": \\\"Value of the answer\\\"\\n    },\\n    \\\"reason\\\": {\\n        \\\"type\\\": \\\"string\\\",\\n        \\\"description\\\": \\\"Reason for the answer\\\"\\n    },\\n    \\\"optional\\\": {\\n        \\\"type\\\": \\\"boolean\\\"\\n    },\\n    \\\"count\\\": {\\n        \\\"type\\\": \\\"number\\\"\\n    },\\n    \\\"children\\\": {\\n        \\\"type\\\": \\\"array\\\",\\n        \\\"items\\\": {\\n            \\\"type\\\": \\\"object\\\",\\n            \\\"properties\\\": {\\n                \\\"value\\\": {\\n                    \\\"type\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"Value of the children's answer\\\"\\n                }\\n            }\\n        }\\n    }\\n}\",\n                \"description\": \"JSON schema for the structured output\",\n                \"optional\": true,\n                \"show\": {\n                  \"llmStructuredOutput[$index].type\": \"jsonArray\"\n                }\n              },\n              {\n                \"label\": \"Description\",\n                \"name\": \"description\",\n                \"type\": \"string\",\n                \"placeholder\": \"Description of the key\"\n              }\n            ],\n            \"id\": \"llmAgentflow_0-input-llmStructuredOutput-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Update Flow State\",\n            \"name\": \"llmUpdateState\",\n            \"description\": \"Update runtime state during the execution of the workflow\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"asyncOptions\",\n                \"loadMethod\": \"listRuntimeStateKeys\",\n                \"freeSolo\": true\n              },\n              {\n                \"label\": \"Value\",\n                \"name\": \"value\",\n                \"type\": \"string\",\n                \"acceptVariable\": true,\n                \"acceptNodeOutputAsVariable\": true\n              }\n            ],\n            \"id\": \"llmAgentflow_0-input-llmUpdateState-array\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"llmModel\": \"chatOpenAI\",\n          \"llmMessages\": [\n            {\n              \"role\": \"system\",\n              \"content\": \"<p>Given the user question and history, construct a short string that can be used for searching vector database. Only generate the query, no meta comments, no explanation</p><p><strong>Example</strong>:</p><p>Question: what are the events happening today?</p><p>Query: today's event</p><p></p><p><strong>Example</strong>:</p><p>Question: how about the address?</p><p>Query: business address of the shop</p><p></p><p>Question: <span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"question\\\" data-label=\\\"question\\\">{{ question }}</span></p><p>Query:</p>\"\n            }\n          ],\n          \"llmEnableMemory\": true,\n          \"llmMemoryType\": \"allMessages\",\n          \"llmUserMessage\": \"\",\n          \"llmReturnResponseAs\": \"userMessage\",\n          \"llmStructuredOutput\": \"\",\n          \"llmUpdateState\": [\n            {\n              \"key\": \"query\",\n              \"value\": \"<p><span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"output\\\" data-label=\\\"output\\\">{{ output }}</span></p>\"\n            }\n          ],\n          \"llmModelConfig\": {\n            \"cache\": \"\",\n            \"modelName\": \"gpt-4o-mini\",\n            \"temperature\": 0.9,\n            \"streaming\": true,\n            \"maxTokens\": \"\",\n            \"topP\": \"\",\n            \"frequencyPenalty\": \"\",\n            \"presencePenalty\": \"\",\n            \"timeout\": \"\",\n            \"strictToolCalling\": \"\",\n            \"stopSequence\": \"\",\n            \"basepath\": \"\",\n            \"proxyUrl\": \"\",\n            \"baseOptions\": \"\",\n            \"allowImageUploads\": \"\",\n            \"imageResolution\": \"low\",\n            \"reasoningEffort\": \"\",\n            \"llmModel\": \"chatOpenAI\"\n          }\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"llmAgentflow_0-output-llmAgentflow\",\n            \"label\": \"LLM\",\n            \"name\": \"llmAgentflow\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"type\": \"agentFlow\",\n      \"width\": 120,\n      \"height\": 146,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 203.99796854914894,\n        \"y\": -282.1573937209969\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"llmAgentflow_1\",\n      \"position\": {\n        \"x\": 187.08813131644484,\n        \"y\": 184.62976654810143\n      },\n      \"data\": {\n        \"id\": \"llmAgentflow_1\",\n        \"label\": \"General Answer\",\n        \"version\": 1,\n        \"name\": \"llmAgentflow\",\n        \"type\": \"LLM\",\n        \"color\": \"#64B5F6\",\n        \"baseClasses\": [\n          \"LLM\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Large language models to analyze user-provided inputs and generate responses\",\n        \"inputParams\": [\n          {\n            \"label\": \"Model\",\n            \"name\": \"llmModel\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"loadConfig\": true,\n            \"id\": \"llmAgentflow_1-input-llmModel-asyncOptions\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Messages\",\n            \"name\": \"llmMessages\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Role\",\n                \"name\": \"role\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"System\",\n                    \"name\": \"system\"\n                  },\n                  {\n                    \"label\": \"Assistant\",\n                    \"name\": \"assistant\"\n                  },\n                  {\n                    \"label\": \"Developer\",\n                    \"name\": \"developer\"\n                  },\n                  {\n                    \"label\": \"User\",\n                    \"name\": \"user\"\n                  }\n                ]\n              },\n              {\n                \"label\": \"Content\",\n                \"name\": \"content\",\n                \"type\": \"string\",\n                \"acceptVariable\": true,\n                \"generateInstruction\": true,\n                \"rows\": 4\n              }\n            ],\n            \"id\": \"llmAgentflow_1-input-llmMessages-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Enable Memory\",\n            \"name\": \"llmEnableMemory\",\n            \"type\": \"boolean\",\n            \"description\": \"Enable memory for the conversation thread\",\n            \"default\": true,\n            \"optional\": true,\n            \"id\": \"llmAgentflow_1-input-llmEnableMemory-boolean\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Memory Type\",\n            \"name\": \"llmMemoryType\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"All Messages\",\n                \"name\": \"allMessages\",\n                \"description\": \"Retrieve all messages from the conversation\"\n              },\n              {\n                \"label\": \"Window Size\",\n                \"name\": \"windowSize\",\n                \"description\": \"Uses a fixed window size to surface the last N messages\"\n              },\n              {\n                \"label\": \"Conversation Summary\",\n                \"name\": \"conversationSummary\",\n                \"description\": \"Summarizes the whole conversation\"\n              },\n              {\n                \"label\": \"Conversation Summary Buffer\",\n                \"name\": \"conversationSummaryBuffer\",\n                \"description\": \"Summarize conversations once token limit is reached. Default to 2000\"\n              }\n            ],\n            \"optional\": true,\n            \"default\": \"allMessages\",\n            \"show\": {\n              \"llmEnableMemory\": true\n            },\n            \"id\": \"llmAgentflow_1-input-llmMemoryType-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Window Size\",\n            \"name\": \"llmMemoryWindowSize\",\n            \"type\": \"number\",\n            \"default\": \"20\",\n            \"description\": \"Uses a fixed window size to surface the last N messages\",\n            \"show\": {\n              \"llmMemoryType\": \"windowSize\"\n            },\n            \"id\": \"llmAgentflow_1-input-llmMemoryWindowSize-number\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Max Token Limit\",\n            \"name\": \"llmMemoryMaxTokenLimit\",\n            \"type\": \"number\",\n            \"default\": \"2000\",\n            \"description\": \"Summarize conversations once token limit is reached. Default to 2000\",\n            \"show\": {\n              \"llmMemoryType\": \"conversationSummaryBuffer\"\n            },\n            \"id\": \"llmAgentflow_1-input-llmMemoryMaxTokenLimit-number\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Input Message\",\n            \"name\": \"llmUserMessage\",\n            \"type\": \"string\",\n            \"description\": \"Add an input message as user message at the end of the conversation\",\n            \"rows\": 4,\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"show\": {\n              \"llmEnableMemory\": true\n            },\n            \"id\": \"llmAgentflow_1-input-llmUserMessage-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Return Response As\",\n            \"name\": \"llmReturnResponseAs\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"User Message\",\n                \"name\": \"userMessage\"\n              },\n              {\n                \"label\": \"Assistant Message\",\n                \"name\": \"assistantMessage\"\n              }\n            ],\n            \"default\": \"userMessage\",\n            \"id\": \"llmAgentflow_1-input-llmReturnResponseAs-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"JSON Structured Output\",\n            \"name\": \"llmStructuredOutput\",\n            \"description\": \"Instruct the LLM to give output in a JSON structured schema\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"string\"\n              },\n              {\n                \"label\": \"Type\",\n                \"name\": \"type\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"String\",\n                    \"name\": \"string\"\n                  },\n                  {\n                    \"label\": \"String Array\",\n                    \"name\": \"stringArray\"\n                  },\n                  {\n                    \"label\": \"Number\",\n                    \"name\": \"number\"\n                  },\n                  {\n                    \"label\": \"Boolean\",\n                    \"name\": \"boolean\"\n                  },\n                  {\n                    \"label\": \"Enum\",\n                    \"name\": \"enum\"\n                  },\n                  {\n                    \"label\": \"JSON Array\",\n                    \"name\": \"jsonArray\"\n                  }\n                ]\n              },\n              {\n                \"label\": \"Enum Values\",\n                \"name\": \"enumValues\",\n                \"type\": \"string\",\n                \"placeholder\": \"value1, value2, value3\",\n                \"description\": \"Enum values. Separated by comma\",\n                \"optional\": true,\n                \"show\": {\n                  \"llmStructuredOutput[$index].type\": \"enum\"\n                }\n              },\n              {\n                \"label\": \"JSON Schema\",\n                \"name\": \"jsonSchema\",\n                \"type\": \"code\",\n                \"placeholder\": \"{\\n    \\\"answer\\\": {\\n        \\\"type\\\": \\\"string\\\",\\n        \\\"description\\\": \\\"Value of the answer\\\"\\n    },\\n    \\\"reason\\\": {\\n        \\\"type\\\": \\\"string\\\",\\n        \\\"description\\\": \\\"Reason for the answer\\\"\\n    },\\n    \\\"optional\\\": {\\n        \\\"type\\\": \\\"boolean\\\"\\n    },\\n    \\\"count\\\": {\\n        \\\"type\\\": \\\"number\\\"\\n    },\\n    \\\"children\\\": {\\n        \\\"type\\\": \\\"array\\\",\\n        \\\"items\\\": {\\n            \\\"type\\\": \\\"object\\\",\\n            \\\"properties\\\": {\\n                \\\"value\\\": {\\n                    \\\"type\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"Value of the children's answer\\\"\\n                }\\n            }\\n        }\\n    }\\n}\",\n                \"description\": \"JSON schema for the structured output\",\n                \"optional\": true,\n                \"show\": {\n                  \"llmStructuredOutput[$index].type\": \"jsonArray\"\n                }\n              },\n              {\n                \"label\": \"Description\",\n                \"name\": \"description\",\n                \"type\": \"string\",\n                \"placeholder\": \"Description of the key\"\n              }\n            ],\n            \"id\": \"llmAgentflow_1-input-llmStructuredOutput-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Update Flow State\",\n            \"name\": \"llmUpdateState\",\n            \"description\": \"Update runtime state during the execution of the workflow\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"asyncOptions\",\n                \"loadMethod\": \"listRuntimeStateKeys\",\n                \"freeSolo\": true\n              },\n              {\n                \"label\": \"Value\",\n                \"name\": \"value\",\n                \"type\": \"string\",\n                \"acceptVariable\": true,\n                \"acceptNodeOutputAsVariable\": true\n              }\n            ],\n            \"id\": \"llmAgentflow_1-input-llmUpdateState-array\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"llmModel\": \"chatOpenAI\",\n          \"llmMessages\": [],\n          \"llmEnableMemory\": true,\n          \"llmMemoryType\": \"allMessages\",\n          \"llmUserMessage\": \"\",\n          \"llmReturnResponseAs\": \"userMessage\",\n          \"llmStructuredOutput\": \"\",\n          \"llmUpdateState\": \"\",\n          \"llmModelConfig\": {\n            \"credential\": \"\",\n            \"modelName\": \"gpt-4o-mini\",\n            \"temperature\": 0.9,\n            \"streaming\": true,\n            \"maxTokens\": \"\",\n            \"topP\": \"\",\n            \"frequencyPenalty\": \"\",\n            \"presencePenalty\": \"\",\n            \"timeout\": \"\",\n            \"strictToolCalling\": \"\",\n            \"stopSequence\": \"\",\n            \"basepath\": \"\",\n            \"proxyUrl\": \"\",\n            \"baseOptions\": \"\",\n            \"allowImageUploads\": \"\",\n            \"imageResolution\": \"low\",\n            \"reasoningEffort\": \"\",\n            \"llmModel\": \"chatOpenAI\"\n          }\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"llmAgentflow_1-output-llmAgentflow\",\n            \"label\": \"LLM\",\n            \"name\": \"llmAgentflow\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"type\": \"agentFlow\",\n      \"width\": 120,\n      \"height\": 146,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 187.08813131644484,\n        \"y\": 184.62976654810143\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"retrieverAgentflow_0\",\n      \"position\": {\n        \"x\": 386.2460529158055,\n        \"y\": 33.61069610194559\n      },\n      \"data\": {\n        \"id\": \"retrieverAgentflow_0\",\n        \"label\": \"Retriever Vector DB\",\n        \"version\": 1,\n        \"name\": \"retrieverAgentflow\",\n        \"type\": \"Retriever\",\n        \"color\": \"#b8bedd\",\n        \"baseClasses\": [\n          \"Retriever\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Retrieve information from vector database\",\n        \"inputParams\": [\n          {\n            \"label\": \"Knowledge (Document Stores)\",\n            \"name\": \"retrieverKnowledgeDocumentStores\",\n            \"type\": \"array\",\n            \"description\": \"Document stores to retrieve information from. Document stores must be upserted in advance.\",\n            \"array\": [\n              {\n                \"label\": \"Document Store\",\n                \"name\": \"documentStore\",\n                \"type\": \"asyncOptions\",\n                \"loadMethod\": \"listStores\"\n              }\n            ],\n            \"id\": \"retrieverAgentflow_0-input-retrieverKnowledgeDocumentStores-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Retriever Query\",\n            \"name\": \"retrieverQuery\",\n            \"type\": \"string\",\n            \"placeholder\": \"Enter your query here\",\n            \"rows\": 4,\n            \"acceptVariable\": true,\n            \"id\": \"retrieverAgentflow_0-input-retrieverQuery-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Output Format\",\n            \"name\": \"outputFormat\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"Text\",\n                \"name\": \"text\"\n              },\n              {\n                \"label\": \"Text with Metadata\",\n                \"name\": \"textWithMetadata\"\n              }\n            ],\n            \"default\": \"text\",\n            \"id\": \"retrieverAgentflow_0-input-outputFormat-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Update Flow State\",\n            \"name\": \"retrieverUpdateState\",\n            \"description\": \"Update runtime state during the execution of the workflow\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"asyncOptions\",\n                \"loadMethod\": \"listRuntimeStateKeys\",\n                \"freeSolo\": true\n              },\n              {\n                \"label\": \"Value\",\n                \"name\": \"value\",\n                \"type\": \"string\",\n                \"acceptVariable\": true,\n                \"acceptNodeOutputAsVariable\": true\n              }\n            ],\n            \"id\": \"retrieverAgentflow_0-input-retrieverUpdateState-array\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"retrieverKnowledgeDocumentStores\": [\n            {\n              \"documentStore\": \"570df92b-087b-4d3b-9462-7a11283454a5:ai paper\"\n            }\n          ],\n          \"retrieverQuery\": \"<p><span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"$flow.state.query\\\" data-label=\\\"$flow.state.query\\\">{{ $flow.state.query }}</span> </p>\",\n          \"outputFormat\": \"text\",\n          \"retrieverUpdateState\": \"\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"retrieverAgentflow_0-output-retrieverAgentflow\",\n            \"label\": \"Retriever\",\n            \"name\": \"retrieverAgentflow\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"type\": \"agentFlow\",\n      \"width\": 127,\n      \"height\": 146,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 386.2460529158055,\n        \"y\": 33.61069610194559\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"conditionAgentAgentflow_1\",\n      \"position\": {\n        \"x\": 645.8327299406507,\n        \"y\": -171.62220890348294\n      },\n      \"data\": {\n        \"id\": \"conditionAgentAgentflow_1\",\n        \"label\": \"Check if docs relevant\",\n        \"version\": 1,\n        \"name\": \"conditionAgentAgentflow\",\n        \"type\": \"ConditionAgent\",\n        \"color\": \"#ff8fab\",\n        \"baseClasses\": [\n          \"ConditionAgent\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Utilize an agent to split flows based on dynamic conditions\",\n        \"inputParams\": [\n          {\n            \"label\": \"Model\",\n            \"name\": \"conditionAgentModel\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"loadConfig\": true,\n            \"id\": \"conditionAgentAgentflow_1-input-conditionAgentModel-asyncOptions\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Instructions\",\n            \"name\": \"conditionAgentInstructions\",\n            \"type\": \"string\",\n            \"description\": \"A general instructions of what the condition agent should do\",\n            \"rows\": 4,\n            \"acceptVariable\": true,\n            \"placeholder\": \"Determine if the user is interested in learning about AI\",\n            \"id\": \"conditionAgentAgentflow_1-input-conditionAgentInstructions-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Input\",\n            \"name\": \"conditionAgentInput\",\n            \"type\": \"string\",\n            \"description\": \"Input to be used for the condition agent\",\n            \"rows\": 4,\n            \"acceptVariable\": true,\n            \"default\": \"<p><span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"question\\\" data-label=\\\"question\\\">{{ question }}</span> </p>\",\n            \"id\": \"conditionAgentAgentflow_1-input-conditionAgentInput-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Scenarios\",\n            \"name\": \"conditionAgentScenarios\",\n            \"description\": \"Define the scenarios that will be used as the conditions to split the flow\",\n            \"type\": \"array\",\n            \"array\": [\n              {\n                \"label\": \"Scenario\",\n                \"name\": \"scenario\",\n                \"type\": \"string\",\n                \"placeholder\": \"User is asking for a pizza\"\n              }\n            ],\n            \"default\": [\n              {\n                \"scenario\": \"Relevant\"\n              },\n              {\n                \"scenario\": \"Irrelevant\"\n              }\n            ],\n            \"id\": \"conditionAgentAgentflow_1-input-conditionAgentScenarios-array\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"conditionAgentModel\": \"chatOpenAI\",\n          \"conditionAgentInstructions\": \"<p>Determine if the document is relevant to user question. User question is <span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"question\\\" data-label=\\\"question\\\">{{ question }}</span></p>\",\n          \"conditionAgentInput\": \"<p><span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"retrieverAgentflow_0\\\" data-label=\\\"retrieverAgentflow_0\\\">{{ retrieverAgentflow_0 }}</span> </p>\",\n          \"conditionAgentScenarios\": [\n            {\n              \"scenario\": \"Relevant\"\n            },\n            {\n              \"scenario\": \"Irrelevant\"\n            }\n          ],\n          \"conditionAgentModelConfig\": {\n            \"credential\": \"\",\n            \"modelName\": \"gpt-4o-mini\",\n            \"temperature\": 0.9,\n            \"streaming\": true,\n            \"maxTokens\": \"\",\n            \"topP\": \"\",\n            \"frequencyPenalty\": \"\",\n            \"presencePenalty\": \"\",\n            \"timeout\": \"\",\n            \"strictToolCalling\": \"\",\n            \"stopSequence\": \"\",\n            \"basepath\": \"\",\n            \"proxyUrl\": \"\",\n            \"baseOptions\": \"\",\n            \"allowImageUploads\": \"\",\n            \"imageResolution\": \"low\",\n            \"reasoningEffort\": \"\",\n            \"conditionAgentModel\": \"chatOpenAI\"\n          }\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"conditionAgentAgentflow_1-output-0\",\n            \"label\": \"Condition Agent\",\n            \"name\": \"conditionAgentAgentflow\"\n          },\n          {\n            \"id\": \"conditionAgentAgentflow_1-output-1\",\n            \"label\": \"Condition Agent\",\n            \"name\": \"conditionAgentAgentflow\"\n          }\n        ],\n        \"outputs\": {\n          \"conditionAgentAgentflow\": \"\"\n        },\n        \"selected\": false\n      },\n      \"type\": \"agentFlow\",\n      \"width\": 140,\n      \"height\": 124,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 645.8327299406507,\n        \"y\": -171.62220890348294\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"llmAgentflow_2\",\n      \"position\": {\n        \"x\": 979.0050663144605,\n        \"y\": -271.41266744535494\n      },\n      \"data\": {\n        \"id\": \"llmAgentflow_2\",\n        \"label\": \"Generate Response\",\n        \"version\": 1,\n        \"name\": \"llmAgentflow\",\n        \"type\": \"LLM\",\n        \"color\": \"#64B5F6\",\n        \"baseClasses\": [\n          \"LLM\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Large language models to analyze user-provided inputs and generate responses\",\n        \"inputParams\": [\n          {\n            \"label\": \"Model\",\n            \"name\": \"llmModel\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"loadConfig\": true,\n            \"id\": \"llmAgentflow_2-input-llmModel-asyncOptions\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Messages\",\n            \"name\": \"llmMessages\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Role\",\n                \"name\": \"role\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"System\",\n                    \"name\": \"system\"\n                  },\n                  {\n                    \"label\": \"Assistant\",\n                    \"name\": \"assistant\"\n                  },\n                  {\n                    \"label\": \"Developer\",\n                    \"name\": \"developer\"\n                  },\n                  {\n                    \"label\": \"User\",\n                    \"name\": \"user\"\n                  }\n                ]\n              },\n              {\n                \"label\": \"Content\",\n                \"name\": \"content\",\n                \"type\": \"string\",\n                \"acceptVariable\": true,\n                \"generateInstruction\": true,\n                \"rows\": 4\n              }\n            ],\n            \"id\": \"llmAgentflow_2-input-llmMessages-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Enable Memory\",\n            \"name\": \"llmEnableMemory\",\n            \"type\": \"boolean\",\n            \"description\": \"Enable memory for the conversation thread\",\n            \"default\": true,\n            \"optional\": true,\n            \"id\": \"llmAgentflow_2-input-llmEnableMemory-boolean\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Memory Type\",\n            \"name\": \"llmMemoryType\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"All Messages\",\n                \"name\": \"allMessages\",\n                \"description\": \"Retrieve all messages from the conversation\"\n              },\n              {\n                \"label\": \"Window Size\",\n                \"name\": \"windowSize\",\n                \"description\": \"Uses a fixed window size to surface the last N messages\"\n              },\n              {\n                \"label\": \"Conversation Summary\",\n                \"name\": \"conversationSummary\",\n                \"description\": \"Summarizes the whole conversation\"\n              },\n              {\n                \"label\": \"Conversation Summary Buffer\",\n                \"name\": \"conversationSummaryBuffer\",\n                \"description\": \"Summarize conversations once token limit is reached. Default to 2000\"\n              }\n            ],\n            \"optional\": true,\n            \"default\": \"allMessages\",\n            \"show\": {\n              \"llmEnableMemory\": true\n            },\n            \"id\": \"llmAgentflow_2-input-llmMemoryType-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Window Size\",\n            \"name\": \"llmMemoryWindowSize\",\n            \"type\": \"number\",\n            \"default\": \"20\",\n            \"description\": \"Uses a fixed window size to surface the last N messages\",\n            \"show\": {\n              \"llmMemoryType\": \"windowSize\"\n            },\n            \"id\": \"llmAgentflow_2-input-llmMemoryWindowSize-number\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Max Token Limit\",\n            \"name\": \"llmMemoryMaxTokenLimit\",\n            \"type\": \"number\",\n            \"default\": \"2000\",\n            \"description\": \"Summarize conversations once token limit is reached. Default to 2000\",\n            \"show\": {\n              \"llmMemoryType\": \"conversationSummaryBuffer\"\n            },\n            \"id\": \"llmAgentflow_2-input-llmMemoryMaxTokenLimit-number\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Input Message\",\n            \"name\": \"llmUserMessage\",\n            \"type\": \"string\",\n            \"description\": \"Add an input message as user message at the end of the conversation\",\n            \"rows\": 4,\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"show\": {\n              \"llmEnableMemory\": true\n            },\n            \"id\": \"llmAgentflow_2-input-llmUserMessage-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Return Response As\",\n            \"name\": \"llmReturnResponseAs\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"User Message\",\n                \"name\": \"userMessage\"\n              },\n              {\n                \"label\": \"Assistant Message\",\n                \"name\": \"assistantMessage\"\n              }\n            ],\n            \"default\": \"userMessage\",\n            \"id\": \"llmAgentflow_2-input-llmReturnResponseAs-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"JSON Structured Output\",\n            \"name\": \"llmStructuredOutput\",\n            \"description\": \"Instruct the LLM to give output in a JSON structured schema\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"string\"\n              },\n              {\n                \"label\": \"Type\",\n                \"name\": \"type\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"String\",\n                    \"name\": \"string\"\n                  },\n                  {\n                    \"label\": \"String Array\",\n                    \"name\": \"stringArray\"\n                  },\n                  {\n                    \"label\": \"Number\",\n                    \"name\": \"number\"\n                  },\n                  {\n                    \"label\": \"Boolean\",\n                    \"name\": \"boolean\"\n                  },\n                  {\n                    \"label\": \"Enum\",\n                    \"name\": \"enum\"\n                  },\n                  {\n                    \"label\": \"JSON Array\",\n                    \"name\": \"jsonArray\"\n                  }\n                ]\n              },\n              {\n                \"label\": \"Enum Values\",\n                \"name\": \"enumValues\",\n                \"type\": \"string\",\n                \"placeholder\": \"value1, value2, value3\",\n                \"description\": \"Enum values. Separated by comma\",\n                \"optional\": true,\n                \"show\": {\n                  \"llmStructuredOutput[$index].type\": \"enum\"\n                }\n              },\n              {\n                \"label\": \"JSON Schema\",\n                \"name\": \"jsonSchema\",\n                \"type\": \"code\",\n                \"placeholder\": \"{\\n    \\\"answer\\\": {\\n        \\\"type\\\": \\\"string\\\",\\n        \\\"description\\\": \\\"Value of the answer\\\"\\n    },\\n    \\\"reason\\\": {\\n        \\\"type\\\": \\\"string\\\",\\n        \\\"description\\\": \\\"Reason for the answer\\\"\\n    },\\n    \\\"optional\\\": {\\n        \\\"type\\\": \\\"boolean\\\"\\n    },\\n    \\\"count\\\": {\\n        \\\"type\\\": \\\"number\\\"\\n    },\\n    \\\"children\\\": {\\n        \\\"type\\\": \\\"array\\\",\\n        \\\"items\\\": {\\n            \\\"type\\\": \\\"object\\\",\\n            \\\"properties\\\": {\\n                \\\"value\\\": {\\n                    \\\"type\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"Value of the children's answer\\\"\\n                }\\n            }\\n        }\\n    }\\n}\",\n                \"description\": \"JSON schema for the structured output\",\n                \"optional\": true,\n                \"show\": {\n                  \"llmStructuredOutput[$index].type\": \"jsonArray\"\n                }\n              },\n              {\n                \"label\": \"Description\",\n                \"name\": \"description\",\n                \"type\": \"string\",\n                \"placeholder\": \"Description of the key\"\n              }\n            ],\n            \"id\": \"llmAgentflow_2-input-llmStructuredOutput-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Update Flow State\",\n            \"name\": \"llmUpdateState\",\n            \"description\": \"Update runtime state during the execution of the workflow\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"asyncOptions\",\n                \"loadMethod\": \"listRuntimeStateKeys\",\n                \"freeSolo\": true\n              },\n              {\n                \"label\": \"Value\",\n                \"name\": \"value\",\n                \"type\": \"string\",\n                \"acceptVariable\": true,\n                \"acceptNodeOutputAsVariable\": true\n              }\n            ],\n            \"id\": \"llmAgentflow_2-input-llmUpdateState-array\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"llmModel\": \"chatOpenAI\",\n          \"llmMessages\": \"\",\n          \"llmEnableMemory\": true,\n          \"llmMemoryType\": \"allMessages\",\n          \"llmUserMessage\": \"<p>Given the question: <span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"question\\\" data-label=\\\"question\\\">{{ question }}</span></p><p>And the findings: <span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"retrieverAgentflow_0\\\" data-label=\\\"retrieverAgentflow_0\\\">{{ retrieverAgentflow_0 }}</span></p><p>Output the final response</p>\",\n          \"llmReturnResponseAs\": \"userMessage\",\n          \"llmStructuredOutput\": \"\",\n          \"llmUpdateState\": \"\",\n          \"llmModelConfig\": {\n            \"cache\": \"\",\n            \"modelName\": \"gpt-4o-mini\",\n            \"temperature\": 0.9,\n            \"streaming\": true,\n            \"maxTokens\": \"\",\n            \"topP\": \"\",\n            \"frequencyPenalty\": \"\",\n            \"presencePenalty\": \"\",\n            \"timeout\": \"\",\n            \"strictToolCalling\": \"\",\n            \"stopSequence\": \"\",\n            \"basepath\": \"\",\n            \"proxyUrl\": \"\",\n            \"baseOptions\": \"\",\n            \"allowImageUploads\": \"\",\n            \"imageResolution\": \"low\",\n            \"reasoningEffort\": \"\",\n            \"llmModel\": \"chatOpenAI\"\n          }\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"llmAgentflow_2-output-llmAgentflow\",\n            \"label\": \"LLM\",\n            \"name\": \"llmAgentflow\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"type\": \"agentFlow\",\n      \"width\": 127,\n      \"height\": 146,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 979.0050663144605,\n        \"y\": -271.41266744535494\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"llmAgentflow_3\",\n      \"position\": {\n        \"x\": 922.1644474867795,\n        \"y\": 164.02211947427529\n      },\n      \"data\": {\n        \"id\": \"llmAgentflow_3\",\n        \"label\": \"Regenerate Question\",\n        \"version\": 1,\n        \"name\": \"llmAgentflow\",\n        \"type\": \"LLM\",\n        \"color\": \"#64B5F6\",\n        \"baseClasses\": [\n          \"LLM\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Large language models to analyze user-provided inputs and generate responses\",\n        \"inputParams\": [\n          {\n            \"label\": \"Model\",\n            \"name\": \"llmModel\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listModels\",\n            \"loadConfig\": true,\n            \"id\": \"llmAgentflow_3-input-llmModel-asyncOptions\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Messages\",\n            \"name\": \"llmMessages\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Role\",\n                \"name\": \"role\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"System\",\n                    \"name\": \"system\"\n                  },\n                  {\n                    \"label\": \"Assistant\",\n                    \"name\": \"assistant\"\n                  },\n                  {\n                    \"label\": \"Developer\",\n                    \"name\": \"developer\"\n                  },\n                  {\n                    \"label\": \"User\",\n                    \"name\": \"user\"\n                  }\n                ]\n              },\n              {\n                \"label\": \"Content\",\n                \"name\": \"content\",\n                \"type\": \"string\",\n                \"acceptVariable\": true,\n                \"generateInstruction\": true,\n                \"rows\": 4\n              }\n            ],\n            \"id\": \"llmAgentflow_3-input-llmMessages-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Enable Memory\",\n            \"name\": \"llmEnableMemory\",\n            \"type\": \"boolean\",\n            \"description\": \"Enable memory for the conversation thread\",\n            \"default\": true,\n            \"optional\": true,\n            \"id\": \"llmAgentflow_3-input-llmEnableMemory-boolean\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Memory Type\",\n            \"name\": \"llmMemoryType\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"All Messages\",\n                \"name\": \"allMessages\",\n                \"description\": \"Retrieve all messages from the conversation\"\n              },\n              {\n                \"label\": \"Window Size\",\n                \"name\": \"windowSize\",\n                \"description\": \"Uses a fixed window size to surface the last N messages\"\n              },\n              {\n                \"label\": \"Conversation Summary\",\n                \"name\": \"conversationSummary\",\n                \"description\": \"Summarizes the whole conversation\"\n              },\n              {\n                \"label\": \"Conversation Summary Buffer\",\n                \"name\": \"conversationSummaryBuffer\",\n                \"description\": \"Summarize conversations once token limit is reached. Default to 2000\"\n              }\n            ],\n            \"optional\": true,\n            \"default\": \"allMessages\",\n            \"show\": {\n              \"llmEnableMemory\": true\n            },\n            \"id\": \"llmAgentflow_3-input-llmMemoryType-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Window Size\",\n            \"name\": \"llmMemoryWindowSize\",\n            \"type\": \"number\",\n            \"default\": \"20\",\n            \"description\": \"Uses a fixed window size to surface the last N messages\",\n            \"show\": {\n              \"llmMemoryType\": \"windowSize\"\n            },\n            \"id\": \"llmAgentflow_3-input-llmMemoryWindowSize-number\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Max Token Limit\",\n            \"name\": \"llmMemoryMaxTokenLimit\",\n            \"type\": \"number\",\n            \"default\": \"2000\",\n            \"description\": \"Summarize conversations once token limit is reached. Default to 2000\",\n            \"show\": {\n              \"llmMemoryType\": \"conversationSummaryBuffer\"\n            },\n            \"id\": \"llmAgentflow_3-input-llmMemoryMaxTokenLimit-number\",\n            \"display\": false\n          },\n          {\n            \"label\": \"Input Message\",\n            \"name\": \"llmUserMessage\",\n            \"type\": \"string\",\n            \"description\": \"Add an input message as user message at the end of the conversation\",\n            \"rows\": 4,\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"show\": {\n              \"llmEnableMemory\": true\n            },\n            \"id\": \"llmAgentflow_3-input-llmUserMessage-string\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Return Response As\",\n            \"name\": \"llmReturnResponseAs\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"label\": \"User Message\",\n                \"name\": \"userMessage\"\n              },\n              {\n                \"label\": \"Assistant Message\",\n                \"name\": \"assistantMessage\"\n              }\n            ],\n            \"default\": \"userMessage\",\n            \"id\": \"llmAgentflow_3-input-llmReturnResponseAs-options\",\n            \"display\": true\n          },\n          {\n            \"label\": \"JSON Structured Output\",\n            \"name\": \"llmStructuredOutput\",\n            \"description\": \"Instruct the LLM to give output in a JSON structured schema\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"string\"\n              },\n              {\n                \"label\": \"Type\",\n                \"name\": \"type\",\n                \"type\": \"options\",\n                \"options\": [\n                  {\n                    \"label\": \"String\",\n                    \"name\": \"string\"\n                  },\n                  {\n                    \"label\": \"String Array\",\n                    \"name\": \"stringArray\"\n                  },\n                  {\n                    \"label\": \"Number\",\n                    \"name\": \"number\"\n                  },\n                  {\n                    \"label\": \"Boolean\",\n                    \"name\": \"boolean\"\n                  },\n                  {\n                    \"label\": \"Enum\",\n                    \"name\": \"enum\"\n                  },\n                  {\n                    \"label\": \"JSON Array\",\n                    \"name\": \"jsonArray\"\n                  }\n                ]\n              },\n              {\n                \"label\": \"Enum Values\",\n                \"name\": \"enumValues\",\n                \"type\": \"string\",\n                \"placeholder\": \"value1, value2, value3\",\n                \"description\": \"Enum values. Separated by comma\",\n                \"optional\": true,\n                \"show\": {\n                  \"llmStructuredOutput[$index].type\": \"enum\"\n                }\n              },\n              {\n                \"label\": \"JSON Schema\",\n                \"name\": \"jsonSchema\",\n                \"type\": \"code\",\n                \"placeholder\": \"{\\n    \\\"answer\\\": {\\n        \\\"type\\\": \\\"string\\\",\\n        \\\"description\\\": \\\"Value of the answer\\\"\\n    },\\n    \\\"reason\\\": {\\n        \\\"type\\\": \\\"string\\\",\\n        \\\"description\\\": \\\"Reason for the answer\\\"\\n    },\\n    \\\"optional\\\": {\\n        \\\"type\\\": \\\"boolean\\\"\\n    },\\n    \\\"count\\\": {\\n        \\\"type\\\": \\\"number\\\"\\n    },\\n    \\\"children\\\": {\\n        \\\"type\\\": \\\"array\\\",\\n        \\\"items\\\": {\\n            \\\"type\\\": \\\"object\\\",\\n            \\\"properties\\\": {\\n                \\\"value\\\": {\\n                    \\\"type\\\": \\\"string\\\",\\n                    \\\"description\\\": \\\"Value of the children's answer\\\"\\n                }\\n            }\\n        }\\n    }\\n}\",\n                \"description\": \"JSON schema for the structured output\",\n                \"optional\": true,\n                \"show\": {\n                  \"llmStructuredOutput[$index].type\": \"jsonArray\"\n                }\n              },\n              {\n                \"label\": \"Description\",\n                \"name\": \"description\",\n                \"type\": \"string\",\n                \"placeholder\": \"Description of the key\"\n              }\n            ],\n            \"id\": \"llmAgentflow_3-input-llmStructuredOutput-array\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Update Flow State\",\n            \"name\": \"llmUpdateState\",\n            \"description\": \"Update runtime state during the execution of the workflow\",\n            \"type\": \"array\",\n            \"optional\": true,\n            \"acceptVariable\": true,\n            \"array\": [\n              {\n                \"label\": \"Key\",\n                \"name\": \"key\",\n                \"type\": \"asyncOptions\",\n                \"loadMethod\": \"listRuntimeStateKeys\",\n                \"freeSolo\": true\n              },\n              {\n                \"label\": \"Value\",\n                \"name\": \"value\",\n                \"type\": \"string\",\n                \"acceptVariable\": true,\n                \"acceptNodeOutputAsVariable\": true\n              }\n            ],\n            \"id\": \"llmAgentflow_3-input-llmUpdateState-array\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"llmModel\": \"chatOpenAI\",\n          \"llmMessages\": [\n            {\n              \"role\": \"system\",\n              \"content\": \"<p>You are a helpful assistant that can transform the query to produce a better question.</p>\"\n            }\n          ],\n          \"llmEnableMemory\": true,\n          \"llmMemoryType\": \"allMessages\",\n          \"llmUserMessage\": \"<p>Look at the input and try to reason about the underlying semantic intent / meaning.</p><p>Here is the initial question:</p><p><span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"$flow.state.query\\\" data-label=\\\"$flow.state.query\\\">{{ $flow.state.query }}</span> </p><p>Formulate an improved question:</p><p></p>\",\n          \"llmReturnResponseAs\": \"userMessage\",\n          \"llmStructuredOutput\": \"\",\n          \"llmUpdateState\": [\n            {\n              \"key\": \"query\",\n              \"value\": \"<p><span class=\\\"variable\\\" data-type=\\\"mention\\\" data-id=\\\"output\\\" data-label=\\\"output\\\">{{ output }}</span> </p>\"\n            }\n          ],\n          \"llmModelConfig\": {\n            \"cache\": \"\",\n            \"modelName\": \"gpt-4o-mini\",\n            \"temperature\": 0.9,\n            \"streaming\": true,\n            \"maxTokens\": \"\",\n            \"topP\": \"\",\n            \"frequencyPenalty\": \"\",\n            \"presencePenalty\": \"\",\n            \"timeout\": \"\",\n            \"strictToolCalling\": \"\",\n            \"stopSequence\": \"\",\n            \"basepath\": \"\",\n            \"proxyUrl\": \"\",\n            \"baseOptions\": \"\",\n            \"allowImageUploads\": \"\",\n            \"imageResolution\": \"low\",\n            \"reasoningEffort\": \"\",\n            \"llmModel\": \"chatOpenAI\"\n          }\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"llmAgentflow_3-output-llmAgentflow\",\n            \"label\": \"LLM\",\n            \"name\": \"llmAgentflow\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"type\": \"agentFlow\",\n      \"width\": 137,\n      \"height\": 146,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 922.1644474867795,\n        \"y\": 164.02211947427529\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"loopAgentflow_0\",\n      \"position\": {\n        \"x\": 1163.2442958690758,\n        \"y\": -58.15971579411427\n      },\n      \"data\": {\n        \"id\": \"loopAgentflow_0\",\n        \"label\": \"Loop back to Retriever\",\n        \"version\": 1,\n        \"name\": \"loopAgentflow\",\n        \"type\": \"Loop\",\n        \"color\": \"#FFA07A\",\n        \"hideOutput\": true,\n        \"baseClasses\": [\n          \"Loop\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Loop back to a previous node\",\n        \"inputParams\": [\n          {\n            \"label\": \"Loop Back To\",\n            \"name\": \"loopBackToNode\",\n            \"type\": \"asyncOptions\",\n            \"loadMethod\": \"listPreviousNodes\",\n            \"freeSolo\": true,\n            \"id\": \"loopAgentflow_0-input-loopBackToNode-asyncOptions\",\n            \"display\": true\n          },\n          {\n            \"label\": \"Max Loop Count\",\n            \"name\": \"maxLoopCount\",\n            \"type\": \"number\",\n            \"default\": 5,\n            \"id\": \"loopAgentflow_0-input-maxLoopCount-number\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"loopBackToNode\": \"retrieverAgentflow_0-Retriever Vector DB\",\n          \"maxLoopCount\": 5\n        },\n        \"outputAnchors\": [],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"type\": \"agentFlow\",\n      \"width\": 140,\n      \"height\": 164,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 1163.2442958690758,\n        \"y\": -58.15971579411427\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"stickyNoteAgentflow_0\",\n      \"position\": {\n        \"x\": 145.5705985486235,\n        \"y\": -116.29641765720946\n      },\n      \"data\": {\n        \"id\": \"stickyNoteAgentflow_0\",\n        \"label\": \"Sticky Note\",\n        \"version\": 1,\n        \"name\": \"stickyNoteAgentflow\",\n        \"type\": \"StickyNote\",\n        \"color\": \"#fee440\",\n        \"baseClasses\": [\n          \"StickyNote\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Add notes to the agent flow\",\n        \"inputParams\": [\n          {\n            \"label\": \"\",\n            \"name\": \"note\",\n            \"type\": \"string\",\n            \"rows\": 1,\n            \"placeholder\": \"Type something here\",\n            \"optional\": true,\n            \"id\": \"stickyNoteAgentflow_0-input-note-string\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"note\": \"First update of the state.query\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"stickyNoteAgentflow_0-output-stickyNoteAgentflow\",\n            \"label\": \"Sticky Note\",\n            \"name\": \"stickyNoteAgentflow\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"type\": \"stickyNote\",\n      \"width\": 210,\n      \"height\": 82,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 145.5705985486235,\n        \"y\": -116.29641765720946\n      },\n      \"dragging\": false\n    },\n    {\n      \"id\": \"stickyNoteAgentflow_1\",\n      \"position\": {\n        \"x\": 923.4413972289242,\n        \"y\": 110.04672879978278\n      },\n      \"data\": {\n        \"id\": \"stickyNoteAgentflow_1\",\n        \"label\": \"Sticky Note (1)\",\n        \"version\": 1,\n        \"name\": \"stickyNoteAgentflow\",\n        \"type\": \"StickyNote\",\n        \"color\": \"#fee440\",\n        \"baseClasses\": [\n          \"StickyNote\"\n        ],\n        \"category\": \"Agent Flows\",\n        \"description\": \"Add notes to the agent flow\",\n        \"inputParams\": [\n          {\n            \"label\": \"\",\n            \"name\": \"note\",\n            \"type\": \"string\",\n            \"rows\": 1,\n            \"placeholder\": \"Type something here\",\n            \"optional\": true,\n            \"id\": \"stickyNoteAgentflow_1-input-note-string\",\n            \"display\": true\n          }\n        ],\n        \"inputAnchors\": [],\n        \"inputs\": {\n          \"note\": \"Second update of state.query\"\n        },\n        \"outputAnchors\": [\n          {\n            \"id\": \"stickyNoteAgentflow_1-output-stickyNoteAgentflow\",\n            \"label\": \"Sticky Note\",\n            \"name\": \"stickyNoteAgentflow\"\n          }\n        ],\n        \"outputs\": {},\n        \"selected\": false\n      },\n      \"type\": \"stickyNote\",\n      \"width\": 210,\n      \"height\": 82,\n      \"selected\": false,\n      \"positionAbsolute\": {\n        \"x\": 923.4413972289242,\n        \"y\": 110.04672879978278\n      },\n      \"dragging\": false\n    }\n  ],\n  \"edges\": [\n    {\n      \"type\": \"agentFlow\",\n      \"source\": \"conditionAgentAgentflow_0\",\n      \"sourceHandle\": \"conditionAgentAgentflow_0-output-0\",\n      \"target\": \"llmAgentflow_0\",\n      \"targetHandle\": \"llmAgentflow_0\",\n      \"data\": {\n        \"sourceColor\": \"#ff8fab\",\n        \"targetColor\": \"#64B5F6\",\n        \"edgeLabel\": \"0\",\n        \"isHumanInput\": false\n      },\n      \"id\": \"conditionAgentAgentflow_0-conditionAgentAgentflow_0-output-0-llmAgentflow_0-llmAgentflow_0\"\n    },\n    {\n      \"type\": \"agentFlow\",\n      \"source\": \"conditionAgentAgentflow_0\",\n      \"sourceHandle\": \"conditionAgentAgentflow_0-output-1\",\n      \"target\": \"llmAgentflow_1\",\n      \"targetHandle\": \"llmAgentflow_1\",\n      \"data\": {\n        \"sourceColor\": \"#ff8fab\",\n        \"targetColor\": \"#64B5F6\",\n        \"edgeLabel\": \"1\",\n        \"isHumanInput\": false\n      },\n      \"id\": \"conditionAgentAgentflow_0-conditionAgentAgentflow_0-output-1-llmAgentflow_1-llmAgentflow_1\"\n    },\n    {\n      \"type\": \"agentFlow\",\n      \"source\": \"startAgentflow_0\",\n      \"sourceHandle\": \"startAgentflow_0-output-startAgentflow\",\n      \"target\": \"conditionAgentAgentflow_0\",\n      \"targetHandle\": \"conditionAgentAgentflow_0\",\n      \"data\": {\n        \"sourceColor\": \"#7EE787\",\n        \"targetColor\": \"#ff8fab\",\n        \"isHumanInput\": false\n      },\n      \"id\": \"startAgentflow_0-startAgentflow_0-output-startAgentflow-conditionAgentAgentflow_0-conditionAgentAgentflow_0\"\n    },\n    {\n      \"type\": \"agentFlow\",\n      \"source\": \"llmAgentflow_0\",\n      \"sourceHandle\": \"llmAgentflow_0-output-llmAgentflow\",\n      \"target\": \"retrieverAgentflow_0\",\n      \"targetHandle\": \"retrieverAgentflow_0\",\n      \"data\": {\n        \"sourceColor\": \"#64B5F6\",\n        \"targetColor\": \"#b8bedd\",\n        \"isHumanInput\": false\n      },\n      \"id\": \"llmAgentflow_0-llmAgentflow_0-output-llmAgentflow-retrieverAgentflow_0-retrieverAgentflow_0\"\n    },\n    {\n      \"type\": \"agentFlow\",\n      \"source\": \"retrieverAgentflow_0\",\n      \"sourceHandle\": \"retrieverAgentflow_0-output-retrieverAgentflow\",\n      \"target\": \"conditionAgentAgentflow_1\",\n      \"targetHandle\": \"conditionAgentAgentflow_1\",\n      \"data\": {\n        \"sourceColor\": \"#b8bedd\",\n        \"targetColor\": \"#ff8fab\",\n        \"isHumanInput\": false\n      },\n      \"id\": \"retrieverAgentflow_0-retrieverAgentflow_0-output-retrieverAgentflow-conditionAgentAgentflow_1-conditionAgentAgentflow_1\"\n    },\n    {\n      \"type\": \"agentFlow\",\n      \"source\": \"llmAgentflow_3\",\n      \"sourceHandle\": \"llmAgentflow_3-output-llmAgentflow\",\n      \"target\": \"loopAgentflow_0\",\n      \"targetHandle\": \"loopAgentflow_0\",\n      \"data\": {\n        \"sourceColor\": \"#64B5F6\",\n        \"targetColor\": \"#FFA07A\",\n        \"isHumanInput\": false\n      },\n      \"id\": \"llmAgentflow_3-llmAgentflow_3-output-llmAgentflow-loopAgentflow_0-loopAgentflow_0\"\n    },\n    {\n      \"type\": \"agentFlow\",\n      \"source\": \"conditionAgentAgentflow_1\",\n      \"sourceHandle\": \"conditionAgentAgentflow_1-output-1\",\n      \"target\": \"llmAgentflow_3\",\n      \"targetHandle\": \"llmAgentflow_3\",\n      \"data\": {\n        \"sourceColor\": \"#ff8fab\",\n        \"targetColor\": \"#64B5F6\",\n        \"edgeLabel\": \"1\",\n        \"isHumanInput\": false\n      },\n      \"id\": \"conditionAgentAgentflow_1-conditionAgentAgentflow_1-output-1-llmAgentflow_3-llmAgentflow_3\"\n    },\n    {\n      \"type\": \"agentFlow\",\n      \"source\": \"conditionAgentAgentflow_1\",\n      \"sourceHandle\": \"conditionAgentAgentflow_1-output-0\",\n      \"target\": \"llmAgentflow_2\",\n      \"targetHandle\": \"llmAgentflow_2\",\n      \"data\": {\n        \"sourceColor\": \"#ff8fab\",\n        \"targetColor\": \"#64B5F6\",\n        \"edgeLabel\": \"0\",\n        \"isHumanInput\": false\n      },\n      \"id\": \"conditionAgentAgentflow_1-conditionAgentAgentflow_1-output-0-llmAgentflow_2-llmAgentflow_2\"\n    }\n  ]\n}",
      "type": "AGENTFLOW"
    }
  ],
  "AssistantFlow": [],
  "AssistantCustom": [],
  "AssistantOpenAI": [],
  "AssistantAzure": [],
  "ChatFlow": [],
  "ChatMessage": [],
  "ChatMessageFeedback": [],
  "CustomTemplate": [],
  "DocumentStore": [],
  "DocumentStoreFileChunk": [],
  "Execution": [],
  "Tool": [],
  "Variable": []
}